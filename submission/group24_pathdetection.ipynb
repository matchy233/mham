{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This is the template for the submission. If you want, you can develop your algorithm in a regular Python script and copy the code here for submission.\n\n# Team members (e-mail, legi):\n# chozhang@student.ethz.ch, 22-945-562\n# minghli@student.ethz.ch, 22-953-293\n# changli@student.ethz.ch, 22-944-474\n","metadata":{"pycharm":{"name":"#%%\n"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from typing import *\nimport os\nimport sys\ncurr_environ = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost')\nif curr_environ != 'Localhost':\n    sys.path.append('/kaggle/input/mobile-health-2023-path-detection')\n    input_dir = '/kaggle/input/mobile-health-2023-path-detection'\nelse:\n    input_dir = os.path.abspath('')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:20:58.993107Z","iopub.execute_input":"2023-05-17T16:20:58.994194Z","iopub.status.idle":"2023-05-17T16:20:59.001451Z","shell.execute_reply.started":"2023-05-17T16:20:58.994148Z","shell.execute_reply":"2023-05-17T16:20:59.000594Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom Lilygo.Recording import Recording\nfrom Lilygo.Dataset import Dataset\nfrom os import listdir\nfrom os.path import isfile, join","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-17T16:21:00.745676Z","iopub.execute_input":"2023-05-17T16:21:00.746324Z","iopub.status.idle":"2023-05-17T16:21:00.751801Z","shell.execute_reply.started":"2023-05-17T16:21:00.746282Z","shell.execute_reply":"2023-05-17T16:21:00.750668Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# progress bar\nfrom tqdm import tqdm\nfrom time import time\n\n# plotting\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# for signal processing and calculations\nfrom scipy import signal\nfrom scipy.integrate import simpson\nfrom scipy import stats\n\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\n\n# load models\nfrom joblib import load\n\n# for tuning parameters\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n# for skeleton\nfrom sklearn.base import BaseEstimator\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:02.733114Z","iopub.execute_input":"2023-05-17T16:21:02.733559Z","iopub.status.idle":"2023-05-17T16:21:02.882113Z","shell.execute_reply.started":"2023-05-17T16:21:02.733510Z","shell.execute_reply":"2023-05-17T16:21:02.880890Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"### signal processing functions ###\ndef parse(signal, ds_freq: float = 20.0, zero_mean: bool = False):\n    \"\"\"downsampling the signal to specific frequency ds_freq, and make the data\n     with zero mean if zero_mean is True\"\"\"\n    ori_time_seq = np.array(signal.timestamps)\n    ori_value_seq = np.array(signal.values)\n    if zero_mean:\n        ori_value_seq = ori_value_seq - np.mean(ori_value_seq)\n    dt = 1./ds_freq\n    time_seq = np.arange(start=np.min(ori_time_seq),\n                         stop=np.max(ori_time_seq),\n                         step=dt)\n    value_seq = np.interp(time_seq, ori_time_seq, ori_value_seq)\n    return value_seq\n\n\ndef bp_filter(value_seq, fp: float = 3, fs: float = 20.0):\n    \"\"\"apply band pass filter to the sequence. fp is the threshold frequency,\n     and fs is the sampling frequency.\"\"\"\n    sos = signal.butter(N=4, Wn=[0.5, fp],\n                        btype='bandpass', fs=fs, output='sos')\n    filtered = signal.sosfilt(sos, value_seq)\n    return filtered\n\n\ndef get_envelop(value_seq,\n                fs: float = 20,\n                half_window_size: float = 0.5,\n                _min: float = 20.,\n                _max: float = 500.):\n    \"\"\"\n    get the envelop as the adaptive local norm of the signal, currently the mode\n     of vector (no negative values). The envelop is calculated by the maximum in\n     a window, half_window_size is the seconds of time. _min and _max for clip.\n    \"\"\"\n    half_win = int(fs*half_window_size)\n    seq = np.concatenate(\n        [np.zeros((half_win,)), value_seq, np.zeros((half_win,))])\n    envelop = np.array([np.max(seq[k-half_win:k+half_win+1])\n                        for k in range(half_win, half_win+len(value_seq))])\n    return np.clip(envelop, _min, _max)\n\ndef lp_filter(value_seq, alpha=0.95):\n    x = value_seq[0]\n    a = []\n    for v in value_seq:\n        x = x * alpha + v * (1-alpha)\n        a.append(x)    \n    return np.array(a)\n\ndef split_fea(t, v, n_split, fn = lambda x: np.percentile(x, 50)):\n    tmin, tmax = np.min(t)-1e-8, np.max(t)+1e-8\n    t_split = np.linspace(tmin, tmax, n_split+1, endpoint=True)\n    outputs = []\n    for i in range(n_split):\n        start, end = t_split[i], t_split[i+1]\n        array = v[(t>start) & (t<end)]\n        fea = fn(array)\n        outputs.append(fea)\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:04.968752Z","iopub.execute_input":"2023-05-17T16:21:04.969207Z","iopub.status.idle":"2023-05-17T16:21:04.988101Z","shell.execute_reply.started":"2023-05-17T16:21:04.969174Z","shell.execute_reply":"2023-05-17T16:21:04.986674Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def compute_sm(ax, ay, az, normalize=False):\n    sm = np.sqrt(np.sum(np.square([ax, ay, az]), axis=0))\n    if normalize:\n        return sm - np.mean(sm)\n    else:\n        return sm\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:07.663148Z","iopub.execute_input":"2023-05-17T16:21:07.663580Z","iopub.status.idle":"2023-05-17T16:21:07.671584Z","shell.execute_reply.started":"2023-05-17T16:21:07.663531Z","shell.execute_reply":"2023-05-17T16:21:07.670666Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def parse_accelerometer(trace: Recording, use_zc=True):\n    if use_zc:\n        ax = parse(trace.data['ax'])\n        ay = parse(trace.data['ay'])\n        az = parse(trace.data['az'])\n    else:\n        ax = trace.data['ax'].values\n        ay = trace.data['ay'].values\n        az = trace.data['az'].values\n    return ax, ay, az\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:09.483278Z","iopub.execute_input":"2023-05-17T16:21:09.484019Z","iopub.status.idle":"2023-05-17T16:21:09.491255Z","shell.execute_reply.started":"2023-05-17T16:21:09.483979Z","shell.execute_reply":"2023-05-17T16:21:09.490156Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Step Count","metadata":{}},{"cell_type":"code","source":"class StepCounter(BaseEstimator):\n    def __init__(self, acc_min=2.,  acc_max=3., acc_height=0.25,\n                 half_window_size=0.5, width=0.5):\n        self.acc_min = acc_min\n        self.acc_max = acc_max\n        self.acc_height = acc_height\n        self.half_window_size = half_window_size\n        self.width = width\n\n    def fit(self, data, labels):\n        # no learning actually, just to fit the estimator interface\n        return self\n\n    def score(self, X, y_true, sample_weight=None, normalize=True) -> float:\n        '''\n        Get the \"score\" of the step counting result. \n        The score is calculated based on how different the step count is from the true values\n        '''\n        y_predicted = self.predict(X)\n        diff = y_predicted - y_true\n        scores = np.zeros(len(diff))\n        for i in range(len(diff)):\n            s = - abs(diff[i])\n            scores[i] = s\n        if normalize:\n            return np.average(scores, weights=sample_weight)\n        elif sample_weight is not None:\n            return np.dot(scores, sample_weight)\n        else:\n            return scores.sum()\n\n    def predict(self, sm):\n        acc_step_counts = self._count_steps(sm,\n                                            _max=self.acc_max,\n                                            _min=self.acc_min,\n                                            _height=self.acc_height,\n                                            half_window_size=self.half_window_size,\n                                            width=self.width)\n        res = int(acc_step_counts)\n        return res\n\n    def _count_steps(self, sm, _max, _min, _height,\n                     half_window_size=0.5, width=0.5):\n\n        # calculate the mode.\n        g_v = sm\n        g_v /= get_envelop(g_v,\n                           half_window_size=half_window_size,\n                           _min=_min,\n                           _max=_max)  # an adaptive local norm\n        # band pass\n        filtered_gv = bp_filter(g_v)\n        # amp 1/4 after filtering, should be amplified 4x.\n        filtered_gv = filtered_gv * (filtered_gv > 0) * 4\n\n        # 0.5 optimal for gyro. not tuned for acc but I am lazy.\n        peaks, _ = signal.find_peaks(filtered_gv,\n                                     height=_height,\n                                     distance=20 * 0.2)\n        # when _min=20 for acc, height=0.01 looks good. sota: _min=1, height=0.25\n        step_count = len(peaks)  # peaks are the steps.\n        return step_count\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:11.058775Z","iopub.execute_input":"2023-05-17T16:21:11.059659Z","iopub.status.idle":"2023-05-17T16:21:11.075335Z","shell.execute_reply.started":"2023-05-17T16:21:11.059619Z","shell.execute_reply":"2023-05-17T16:21:11.074034Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Smart Watch Location","metadata":{}},{"cell_type":"code","source":"def encode_feat_vec(sm):\n    \"\"\" return a list containing feature vectors of all the 10s non-overlapping windows\"\"\"\n    WINDOW = 2000  # 10s window\n    feat_vecs = []\n    df1s = []\n    df2s = []\n    dfb2s = []\n    sf = 50.0\n    for i in range(0, len(sm) - WINDOW, WINDOW):\n        data = sm[i:i+WINDOW]\n        low1 = 0.3\n        high1 = 15\n        low2 = 0.6\n        high2 = 2.5\n        nperseg = 2 / low1 * sf\n        freqs, psd = signal.welch(data, sf, nperseg=nperseg)\n        BAND1 = np.logical_and(freqs >= low1, freqs <= high1)\n        BAND2 = np.logical_and(freqs >= low2, freqs <= high2)\n        freq_res = freqs[1] - freqs[0]\n        # compuate total power\n        total_power = simpson(psd[BAND1], dx=freq_res)\n\n        # compute dominant frequencies for BAND1, BAND2\n        idx_p1 = psd[BAND1].argsort()[-1]\n        df1 = low1 + freq_res * idx_p1\n        pdf1 = simpson(psd[BAND1][idx_p1:idx_p1+2], dx=freq_res)\n        df1s.append(df1)\n\n        idx_p2 = psd[BAND1].argsort()[-2]\n        df2 = low1 + freq_res * idx_p2\n        pdf2 = simpson(psd[BAND1][idx_p2:idx_p2+2], dx=freq_res)\n        df2s.append(df2)\n\n        idx_b2 = psd[BAND2].argsort()[-1]\n        dfb2 = low2 + freq_res * idx_b2\n        pdfb2 = simpson(psd[BAND2][idx_b2:idx_b2+2], dx=freq_res)\n        dfb2s.append(dfb2)\n\n        ratio_pdf1_tot = pdf1 / total_power\n\n        if i != 0:\n            ratio_curr_prev_df1 = df1s[-1] / df1s[-2]\n            ratio_curr_prev_df2 = df2s[-1] / df2s[-2]\n            ratio_curr_prev_dfb2 = dfb2s[-1] / dfb2s[-2]\n        else:\n            ratio_curr_prev_df1, ratio_curr_prev_df2, ratio_curr_prev_dfb2 = \\\n                1.0, 1.0, 1.0\n\n        R1 = simpson(psd[freqs <= 3], dx=freq_res) / total_power\n        R2 = simpson(psd[freqs > 3], dx=freq_res) / total_power\n\n        feat_vec = [\n            np.mean(data),\n            np.std(data),\n            df1, pdf1, df2, pdf2, dfb2, pdfb2,\n            ratio_pdf1_tot,\n            ratio_curr_prev_df1, ratio_curr_prev_df2, ratio_curr_prev_dfb2,\n            R1, R2]\n        feat_vecs.append(feat_vec)\n    return feat_vecs\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:13.488261Z","iopub.execute_input":"2023-05-17T16:21:13.489556Z","iopub.status.idle":"2023-05-17T16:21:13.508595Z","shell.execute_reply.started":"2023-05-17T16:21:13.489488Z","shell.execute_reply":"2023-05-17T16:21:13.507329Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# decision tree version\nclass WatchLocDetector(BaseEstimator):\n    def __init__(self):\n        self.clf = tree.DecisionTreeClassifier()\n\n    def fit(self, X, y):\n        selector = np.array([fv[1] > 0.1 for fv in X])\n        _X = X[selector]\n        if isinstance(y, int):\n            _y = [y] * len(_data)\n        else:\n            _y = y[selector]\n        self.clf.fit(_X, _y)\n\n    def score(self, X, y, sample_weight=None):\n        from sklearn.metrics import accuracy_score\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n\n    def predict_fv(self, X):\n        win_res = self.clf.predict([X])\n        return stats.mode(win_res).mode[0]\n\n    def predict(self, traces):\n        # assume array\n        if hasattr(traces, '__len__'):\n            res = np.zeros(len(traces), dtype=int)\n            _traces = traces\n        else:\n            res = np.zeros(1, dtype=int)\n            _traces = [traces]\n        i = 0\n        for trace in _traces:\n            if isinstance(trace, Recording):\n                ax, ay, az = parse_accelerometer(trace)\n                sm = compute_sm(ax, ay, az, normalize=True)\n                feat_vec = encode_feat_vec(sm)\n            else:\n                feat_vec = trace\n            win_res = self.clf.predict(feat_vec)\n            res[i] = stats.mode(win_res).mode[0]\n            i += 1\n        if len(res) == 1:\n            return res[0]\n        else:\n            return res\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:15.835855Z","iopub.execute_input":"2023-05-17T16:21:15.836249Z","iopub.status.idle":"2023-05-17T16:21:15.850192Z","shell.execute_reply.started":"2023-05-17T16:21:15.836220Z","shell.execute_reply":"2023-05-17T16:21:15.849016Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# Activity Recognition","metadata":{}},{"cell_type":"code","source":"class StandStillDetector(BaseEstimator):\n    def __init__(self, mean_thresh = -0.1, std_thresh = 0.17, cont_thresh = 3):\n        # no params, for now\n        self.mean_thresh = mean_thresh\n        self.std_thresh  = std_thresh\n        self.cont_thresh = cont_thresh\n    \n    def fit(self, X, y):\n        # no learning actually\n        return self\n\n    def score(self, X: Sequence[Dict], y, sample_weight=None):\n        from sklearn.metrics import accuracy_score\n        # assume X is traces and y is labels\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n\n    def predict_fv(self, feat_vec):\n        clf_as_still = False\n        prev_still = False\n        cont = 0\n        for fv in feat_vec:\n            if fv[1] < self.std_thresh and prev_still == False:\n                prev_still = True\n                cont = 1\n                if cont >= self.cont_thresh:\n                    clf_as_still = True\n                    break\n            elif fv[1] < self.std_thresh and prev_still == True:\n                cont += 1\n                if cont >= self.cont_thresh:\n                    clf_as_still = True\n                    break\n            else:\n                prev_still = False\n                cont = 0\n        return clf_as_still\n\n    def predict(self, traces):\n        # assume array\n        if hasattr(traces, '__len__'):\n            res = np.zeros(len(traces), dtype=int)\n            _traces = traces\n        else:\n            res = np.zeros(1, dtype=int)\n            _traces = [traces]\n        i = 0\n        for trace in _traces:\n            ax, ay, az = trace['ax'], trace['ay'], trace['az']\n            sm = compute_sm(ax, ay, az, normalize=True)\n            feat_vec = encode_feat_vec(sm)\n            clf_as_still = False\n            prev_still = False\n            cont = 0\n            for fv in feat_vec:\n                if fv[1] < self.std_thresh and prev_still == False:\n                    prev_still = True\n                    cont = 1\n                    if cont >= self.cont_thresh:\n                        clf_as_still = True\n                        break\n                elif fv[1] < self.std_thresh and prev_still == True:\n                    cont += 1\n                    if cont >= self.cont_thresh:\n                        clf_as_still = True\n                        break\n                else:\n                    prev_still = False\n                    cont = 0\n            res[i] = clf_as_still\n            i += 1\n        if len(res) == 1:\n            return res[0]\n        else:\n            return res","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:17.853091Z","iopub.execute_input":"2023-05-17T16:21:17.853465Z","iopub.status.idle":"2023-05-17T16:21:17.871571Z","shell.execute_reply.started":"2023-05-17T16:21:17.853435Z","shell.execute_reply":"2023-05-17T16:21:17.870270Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class ActivityRecognizer(BaseEstimator):\n    def __init__(self, count_tresh=2):\n        self.clf = tree.DecisionTreeClassifier()\n        self.count_thresh = count_tresh\n\n    def fit(self, X, y): # X is an array of windows\n        selector = np.array([fv[1] > 0.1 for fv in X])\n        _X = X[selector]\n        if isinstance(y, int):\n            _y = [y] * len(_X)\n        else:\n            _y = y[selector]\n        self.clf.fit(_X, _y)\n\n    def score(self, X, y, sample_weight=None):\n        y_predict = self.predict(X)\n        tot_score = 0\n        for true, pred in zip(y, y_predict):\n            # walk 1, run 2, cycle 3\n            # 0.05, 0.05, 0.1\n            true_v = [1 in true, 2 in true, 3 in true]\n            pred_v = [1 in pred, 2 in pred, 3 in pred]\n            score = int(true_v[0] == pred_v[0]) + \\\n                    int(true_v[1] == pred_v[1]) + \\\n                    int(true_v[2] == pred_v[2])\n            score /= 4\n            tot_score += score\n        return tot_score / len(y_predict)\n\n    def predict_fv(self, X):\n        win_res = self.clf.predict(X)\n        values, counts = np.unique(win_res, return_counts=True)\n        return [values[j] \n                for j, count in enumerate(counts) if count >= self.count_thresh]\n\n    def predict(self, traces):\n        # assume array\n        if hasattr(traces, '__len__'):\n            res = [0] * len(traces)\n            _traces = traces\n        else:\n            res = [0]\n            _traces = [traces]\n        i = 0\n        for trace in _traces:\n            if isinstance(trace, Recording):\n                ax, ay, az = parse_accelerometer(trace, use_zc=False)\n                sm = compute_sm(ax, ay, az, normalize=True)\n                feat_vec = encode_feat_vec(sm)\n            else:\n                feat_vec = trace\n            # will get an activity prediction for each window\n            win_res = self.clf.predict(feat_vec) \n            values, counts = np.unique(win_res, return_counts=True)\n            res[i] = [values[j] \n                      for j, count in enumerate(counts) if count >= self.count_thresh]\n            i += 1\n        return res\n#         if len(res) == 1:\n#             return res[0]\n#         else:\n#             return res\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:20.374631Z","iopub.execute_input":"2023-05-17T16:21:20.375023Z","iopub.status.idle":"2023-05-17T16:21:20.392982Z","shell.execute_reply.started":"2023-05-17T16:21:20.374993Z","shell.execute_reply":"2023-05-17T16:21:20.391264Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## Path Detection","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.transform import Rotation\n\ndef madgwick_update(q, gyro, accel, mag, beta, dt):\n    q = np.array([q[0], q[1], q[2], q[3]])\n\n    f_g = np.array([2 * (q[1] * q[3] - q[0] * q[2]) - accel[0],\n                    2 * (q[0] * q[1] + q[2] * q[3]) - accel[1],\n                    2 * (0.5 - q[1]**2 - q[2]**2) - accel[2]])\n\n    f_b = np.array([2 * (q[1] * q[2] + q[0] * q[3]) - mag[0],\n                    2 * (q[0] * q[1] - q[2] * q[3]) - mag[1],\n                    2 * (q[0]**2 + q[3]**2 - 0.5) - mag[2]])\n\n    j_g = np.array([[-2 * q[2], 2 * q[3], -2 * q[0], 2 * q[1]],\n                    [2 * q[1],  2 * q[0],  2 * q[3], 2 * q[2]],\n                    [0,         -4 * q[1], -4 * q[2],0]])\n\n    j_b = np.array([[2 * q[3],  2 * q[2],  2 * q[1], 2 * q[0]],\n                    [2 * q[0],  -2 * q[1], -2 * q[2],2 * q[3]],\n                    [-4 * q[0], -4 * q[3],  0,       0]])\n\n    step_size = beta * dt\n    q += step_size * (j_g.T @ f_g + j_b.T @ f_b)\n    q /= np.linalg.norm(q)\n    return q\n\nclass PathDetector(BaseEstimator):\n    # hack labels via GPS, delete later\n    def __init__(self):\n        pass\n    \n    def extract(self,trace):\n        data = trace.data\n        fea_alti = self.get_fea_alti(data['altitude'])\n        fea_ori = self.get_fea_ori(data['ax'],data['ay'],data['az'],data['gx'],data['gy'],data['gz'],data['mx'],data['my'],data['mz'])\n        fea = fea_alti + fea_ori\n        return fea\n\n    # required\n    def fit(self, data, labels):\n        self.clf = RandomForestClassifier(min_samples_leaf=10, max_depth=15, max_features=12)\n        self.clf.fit(data, labels)\n        print('fit score', self.clf.score(data,labels))\n\n    # required    \n    def predict(self, feature):\n        fea = [feature]\n        res = self.clf.predict(fea)[0]\n        return res\n    \n    def get_fea_alti(self, alti):\n        t, alti = parse(alti)\n        t = t[600:]\n        alti = alti[600:]\n        alti = lp_filter(alti, .998)\n        alti = alti - np.min(alti)\n        alti[alti>60] = 60\n        alti /= 60.0\n        t = (t-np.min(t)) / (np.max(t)-np.min(t))\n        fea_alti = split_fea(t, alti, 5, lambda x: np.nanpercentile(x,50))\n        return fea_alti\n    \n    def get_fea_ori(self, ax, ay, az, gx, gy, gz, mx, my, mz):\n        deg2rad = np.pi/180\n        t, ax = parse(ax)\n        t, ay = parse(ay)\n        t, az = parse(az)\n        t, gx = parse(gx)\n        t, gy = parse(gy)\n        t, gz = parse(gz)\n        t, mx = parse(mx)\n        t, my = parse(my)\n        t, mz = parse(mz)\n        gx, gy, gz = gx * deg2rad, gy * deg2rad, gz * deg2rad\n        acc = np.array([ax,ay,az])\n        gyro = np.array([gx,gy,gz])\n        mag = np.array([mx,my,mz])\n\n        acc /= (1e-8 + np.linalg.norm(acc,ord=2,axis=0,keepdims=True))\n        mag /= (1e-8 + np.linalg.norm(mag,ord=2,axis=0,keepdims=True))\n\n        dt = t[1] - t[0]\n        beta = 0.1  # Madgwick filter gain\n        \n        ref_acc_vector = np.array([[0, 0, 1],[0, 0, 1],[0, 0, 1]])  # a - g for a = 0\n        rotation_matrix = Rotation.align_vectors(ref_acc_vector, [acc[:,0],acc[:,1],acc[:,2]])[0].as_matrix()\n        q = Rotation.from_matrix(rotation_matrix).as_quat()\n        \n        quats = []\n        yaws = []\n        for i in range(len(t)):\n            q = madgwick_update(q, gyro[:,i], acc[:,i], mag[:,i], beta, dt)\n            quats.append(q)\n            try:\n                yaw = Rotation.from_quat(q).as_euler('xyz')[-1]\n            except: \n                yaw = 0\n            yaws.append(yaw)\n        t, yaws = np.array(t[400:]), np.array(yaws[400:])\n        yaws -= yaws[0]  # sometime can jump 2pi\n        sinyaws, cosyaws = np.sin(yaws), np.cos(yaws)\n        sinyaws, cosyaws = lp_filter(sinyaws, .97), lp_filter(cosyaws, .97)\n        fea_sinyaw = split_fea(t, sinyaws, 10)\n        fea_cosyaw = split_fea(t, cosyaws, 10)\n        fea_ori = fea_sinyaw + fea_cosyaw\n        return fea_ori\n\npath_detector = PathDetector()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:23.263265Z","iopub.execute_input":"2023-05-17T16:21:23.263691Z","iopub.status.idle":"2023-05-17T16:21:23.298565Z","shell.execute_reply.started":"2023-05-17T16:21:23.263657Z","shell.execute_reply":"2023-05-17T16:21:23.297002Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Get the path of all traces and encode features\n# dir_traces = '/kaggle/input/mobile-health-2023-path-detection/data/test'\n# filenames = [join(dir_traces, f) for f in listdir(dir_traces) if isfile(join(dir_traces, f))]\n# filenames.sort()\n\n# pathdec_fea_test = []\n# for i, filename in enumerate(filenames):\n#     trace = Recording(filename, no_labels=True, mute=True)\n#     pathdec_fea_test.append(path_detector.extract(trace))\n#     print(filename, end = '   \\r')\n# pathdec_fea_test = np.array(pathdec_fea_test)\n# print(pathdec_fea_test.shape)\n\n# np.save('pathfea_test.npy', pathdec_fea_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pathdec_fea_test = np.load('/kaggle/input/activity-recognition-matchy/pathfea_test.npy')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:31.396993Z","iopub.execute_input":"2023-05-17T16:21:31.397430Z","iopub.status.idle":"2023-05-17T16:21:31.410899Z","shell.execute_reply.started":"2023-05-17T16:21:31.397389Z","shell.execute_reply":"2023-05-17T16:21:31.410019Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"features = np.load('/kaggle/input/activity-recognition-matchy/pathfea.npy')\nlabels = np.load('/kaggle/input/activity-recognition-matchy/pathlabels.npy')\n\n# clf = RandomForestClassifier(min_samples_leaf=10, max_depth=15, max_features=12)\n# scores = cross_val_score(clf, features, labels, cv=5, scoring='accuracy')\n# print(scores, np.mean(scores), np.std(scores))\npath_detector.fit(features, labels)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:33.342569Z","iopub.execute_input":"2023-05-17T16:21:33.343569Z","iopub.status.idle":"2023-05-17T16:21:33.695766Z","shell.execute_reply.started":"2023-05-17T16:21:33.343502Z","shell.execute_reply":"2023-05-17T16:21:33.694357Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"fit score 0.8479087452471483\n","output_type":"stream"}]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# Get the path of all traces\ndir_traces = '/kaggle/input/mobile-health-2023-path-detection/data/test'\nfilenames = [join(dir_traces, f) for f in listdir(dir_traces) if isfile(join(dir_traces, f))]\nfilenames.sort()","metadata":{"collapsed":false,"pycharm":{"is_executing":true,"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-17T16:21:37.515950Z","iopub.execute_input":"2023-05-17T16:21:37.516376Z","iopub.status.idle":"2023-05-17T16:21:37.562036Z","shell.execute_reply.started":"2023-05-17T16:21:37.516335Z","shell.execute_reply":"2023-05-17T16:21:37.560571Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"step_counter = StepCounter()\nstandstill_detector = StandStillDetector()\nwatchloc_detector = WatchLocDetector()\nwatchloc_detector.clf = load('/kaggle/input/activity-recognition-matchy/watchloc_detector.joblib')\nactivity_recognizer = ActivityRecognizer()\nactivity_recognizer.clf = load('/kaggle/input/activity-recognition-matchy/activity_recognition.joblib')","metadata":{"execution":{"iopub.status.busy":"2023-05-17T16:21:40.557578Z","iopub.execute_input":"2023-05-17T16:21:40.558016Z","iopub.status.idle":"2023-05-17T16:21:40.576352Z","shell.execute_reply.started":"2023-05-17T16:21:40.557981Z","shell.execute_reply":"2023-05-17T16:21:40.575282Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Loop through all traces and calculate the step count for each trace\nsolution_file = []\nfor i, filename in tqdm(enumerate(filenames)):\n    trace = Recording(filename, no_labels=True, mute=True)\n    categorization_results = {'watch_loc': 0, 'path_idx': 114514,\n                              'step_count': 0, 'stand': 0, 'walk': 0, 'run': 0, 'cycle': 0}\n    ax, ay, az = parse_accelerometer(trace)\n    sm = compute_sm(ax, ay, az)\n    fv = encode_feat_vec(compute_sm(\n        *parse_accelerometer(trace, use_zc=False), normalize=True))\n\n    #\n    # Your algorithm goes here\n    # You can access the variable 'watch_loc' in the dictionary 'categorization_results' for example with\n    # categorization_results['watch_loc'] = 1\n    # Make sure, you do not use the gps data and are tolerant for missing data (see task set).\n    # Your program must not crash when single smartphone data traces are missing.\n    #\n    categorization_results['watch_loc'] = watchloc_detector.predict_fv(fv)\n    categorization_results['step_count'] = step_counter.predict(sm)\n    categorization_results['stand'] = int(standstill_detector.predict_fv(fv))\n    res = activity_recognizer.predict_fv(fv)\n    if 1 in res:\n        categorization_results['walk'] = 1\n    if 2 in res:\n        categorization_results['run'] = 1\n    if 3 in res:\n        categorization_results['cycle'] = 1\n    categorization_results['path_idx'] = path_detector.predict(pathdec_fea_test[i])\n\n    # Append your calculated results and the id of each trace and category to the solution file\n    trace_id = ''.join([*filename][-8:-5])\n    for counter_label, category in enumerate(categorization_results):\n        solution_file.append(\n            [trace_id + f'_{counter_label+1}', categorization_results[category]])\n","metadata":{"collapsed":false,"pycharm":{"is_executing":true,"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-17T16:22:14.958354Z","iopub.execute_input":"2023-05-17T16:22:14.958776Z","iopub.status.idle":"2023-05-17T16:23:49.919449Z","shell.execute_reply.started":"2023-05-17T16:22:14.958746Z","shell.execute_reply":"2023-05-17T16:23:49.917732Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"26it [01:34,  3.65s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m solution_file \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, filename \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(filenames)):\n\u001b[0;32m----> 4\u001b[0m     trace \u001b[38;5;241m=\u001b[39m \u001b[43mRecording\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     categorization_results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwatch_loc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_idx\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m114514\u001b[39m,\n\u001b[1;32m      6\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstand\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwalk\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcycle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m      7\u001b[0m     ax, ay, az \u001b[38;5;241m=\u001b[39m parse_accelerometer(trace)\n","File \u001b[0;32m/kaggle/input/mobile-health-2023-path-detection/Lilygo/Recording.py:73\u001b[0m, in \u001b[0;36mRecording.__init__\u001b[0;34m(self, filename, no_labels, mute)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# Will hold a dictionary directly read from the JSON file\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetAllData()\n","File \u001b[0;32m/kaggle/input/mobile-health-2023-path-detection/Lilygo/Recording.py:109\u001b[0m, in \u001b[0;36mRecording.readFile\u001b[0;34m(self, filename, no_labels, mute)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_str[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m file_str[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    108\u001b[0m         file_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_str\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n","File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n","File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Write the detected step counts into a .csv file to then upload the .csv file to Kaggle\n# When cross-checking the .csv file on your computer, we recommend using the text editor and NOT excel so that the results are displayed correctly\n# IMPORTANT: Do NOT change the name of the columns ('Id' and 'Category') of the .csv file\nsubmission_file_df = pd.DataFrame(np.asarray(solution_file), columns=['Id', 'Category'])\nsubmission_file_df.to_csv('/kaggle/working/submission.csv', header=['Id', 'Category'], index=False)","metadata":{"collapsed":false,"pycharm":{"is_executing":true,"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-17T16:21:52.465431Z","iopub.status.idle":"2023-05-17T16:21:52.466490Z","shell.execute_reply.started":"2023-05-17T16:21:52.466263Z","shell.execute_reply":"2023-05-17T16:21:52.466285Z"},"trusted":true},"execution_count":null,"outputs":[]}]}